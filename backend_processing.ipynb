{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6f6064-b370-4154-8fc9-f5aad10cdacb",
   "metadata": {},
   "source": [
    "# Building a simple chatbot with Retrieval Augmented Generation (RAG) using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58c520-fe45-43a6-85ee-b49e525e5b58",
   "metadata": {},
   "source": [
    "## Overview of RAG and Model Fine-Tuning in ChatGPT Customization\n",
    "\n",
    "- This project leverages the architecture behind ChatGPT, a transformative generative AI that has significantly altered user interactions for information retrieval. The development of a tailored chatbot using ChatGPT's Large Language Model (LLM) incorporates two key methodologies: Retrieval Augmented Generation (RAG) and Fine-tuning of the model.\n",
    "\n",
    "- RAG enhances the LLM's knowledge base by integrating external information retrieval, thus expanding its reference scope. Simultaneously, fine-tuning tailors the LLM's response patterns to specific fields or subjects by training it with a targeted dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55df4a-44bb-4a8a-9fb6-92501c7bbdd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installing the required packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a039957-b567-4961-9d4f-a64e42418921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install openai\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install pypdf\n",
    "# !pip install optimum[exporters]\n",
    "# !pip install InstructorEmbedding\n",
    "# !pip install sentence_transformers\n",
    "# !pip install python-dotenv\n",
    "# !pip install ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ee8a5-7af2-45ea-930d-87f859506a0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Retrieval Augmented Generation(RAG)](https://communitykeeper-media.s3.amazonaws.com/media/images/Knowledge_Indexing_Complete.original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f763f9-06f9-4f85-ab15-4926e34b88ef",
   "metadata": {},
   "source": [
    "## Import libraries, API and set filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cbe370-a693-493c-bd9d-9ad80a257a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document, GPTVectorStoreIndex, ServiceContext\n",
    "from llama_index.readers import BeautifulSoupWebReader, SimpleDirectoryReader\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index import download_loader # For CSV\n",
    "from llama_index import (VectorStoreIndex,\n",
    "                         SimpleDirectoryReader,\n",
    "                         StorageContext,\n",
    "                         load_index_from_storage,)\n",
    "from llama_index import (ServiceContext, \n",
    "                         LLMPredictor, \n",
    "                         OpenAIEmbedding, \n",
    "                         PromptHelper)\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index import set_global_service_context\n",
    "# from llama_index.llama_dataset.generator import RagDatasetGenerator\n",
    "# from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import nest_asyncio\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687537b5-d096-4c9b-ad9c-e013fdbf1386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set filepath to my data directory \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, \"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2fe4b-d17e-46ba-93ec-d4122dc655bc",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "According to [LlamaIndex's documentation](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/simple_directory_reader.html), the `SimpleDirectoryReader` is the most commonly used data connector that just works. Simply pass in a input directory or a list of files. It will select the best file reader based on the file extensions. \n",
    "\n",
    "In this use case here, there are PDFs and html pages from the latest release from MUIS and other islamic wesbites, which are not included in gpt-3.5-turbo's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9282ea0-148b-40db-ae7f-542028a252c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e7b13d2e-c6b2-4aed-9c26-bd3074ff18d8', '302207f2-e6b8-42d0-bcae-3a46b76e9375', '00ce143a-179e-4243-84bd-3d01d8567947', 'a8292f64-b07c-4e8d-944f-847dba457987', '8956786d-a91b-45e3-9e22-53286213c4dd', '76af42c6-f6a9-4fe5-b88f-68681f13e8d1', '61079b6c-5098-4cff-86cc-f33f4dd76a3f', '0e154704-1b79-4a70-9f58-1dc09c977fe4', '914ac300-e4f5-4d79-a16a-5a404c4c1f48', '80f7fd16-a4a7-4882-9cfe-4068a1e4908b', '8e33acac-afaa-4665-867d-80c9d4e3fca3', '4a4aa7fc-ef66-4037-ba83-e1371b6129cb', '41f5adb8-a442-4c87-b380-3fba0aa3256d', '3b404a9d-61f4-467b-9d00-7347f9687849', '211ff184-6646-4ea8-b061-b30a3a8301be', 'e79a9371-c89e-452f-8722-7485c5bb792f', '6c189782-3423-4469-b256-a367d253dd6f', '20f2803c-f10b-4004-b095-3877fdc8331e', '02d69580-f351-4f30-aac3-10e13213b26b', 'f1f1cd96-db05-49bc-8f7b-4b07b53a1b38', '8b3d6223-0dbd-43a9-aedb-cb4162a19db4', 'fdb78fa6-b376-4734-a648-85da80834fc9', '2997c63f-5377-4f01-b3e9-19a7c44fa691', '2201564b-db43-43a6-821b-7f2ec1e7c52b', '0a96f29b-0d0d-4130-9771-7f0020207599', 'a76a5257-7506-47f3-9038-409800303f0f', '1794b0b0-5b99-417c-a28c-be2834a39b77', 'fd26503e-b1a8-4c6c-8c84-f68f77db0c7e', '964d02b7-14dd-4553-acad-f6ff7915d825', 'db63bb6c-353b-48e6-99a5-85eb88bc9144', '3b9c09d6-e056-444d-8846-ce67067c73ac', 'dfae5c2b-38ea-44cd-ba5f-d77727f3c55e', '0c3abc97-3ee2-4ab7-8e33-43336815e0fb', '9affdf5d-718e-4ffc-b103-b52300e18add', '6ca5e6d8-099d-4196-882a-2d49b6a147a2', 'fc758334-9b44-40fe-987a-c327589fb3a6', 'b51feaaa-8a4b-4cf9-a009-3ce759dad13f', 'ff5304d3-e096-4e3b-8c92-da2c05591ee5', '859110c7-78dd-45f3-a639-53d92f229c44', '71954a30-f588-499e-8d19-2f2d117bc2e3', '9e81bc44-298e-4a8c-aac8-3b2299fb694b', '878dfb56-3507-46ee-b8c7-e13d855edc01', 'c755c3b7-4432-486d-9b5a-64a6734f03a9', 'c76fdf3d-a454-46e1-b33e-4aaa0d4483db', '1691a8b4-97db-4c2e-9532-4b2852c98537', 'fd334c08-5947-4c1a-a83a-3addde41077e', '50082fcb-30f1-45ea-83d8-6d6eb275d7a8', '57c5bb7d-d3a9-4459-a180-de9484cda204', 'f9a91ea1-8c87-4985-a232-2fece4fb50b1', '43f4fe9e-798a-4410-9730-9fcac9108465', 'c6a1e21c-cccc-4707-a812-078ff71ab200']\n"
     ]
    }
   ],
   "source": [
    "filename_fn = lambda filename: {'file_name': filename}\n",
    "pdfhtml_docs = SimpleDirectoryReader(input_dir=data_dir, exclude_hidden=True, file_metadata=filename_fn).load_data()\n",
    "print([x.doc_id for x in pdfhtml_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12628f87-30ba-48f4-bcf2-6cba4719fdc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 51 documents.\n"
     ]
    }
   ],
   "source": [
    "# Check if documents are loaded\n",
    "if not pdfhtml_docs:\n",
    "    print(\"No documents loaded. Check your data directory path.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(pdfhtml_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e9021-78e5-45d1-b885-49b8eb2d8540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Use this code if your data set is in CSV format\n",
    "# PagedCSVReader = download_loader(\"PagedCSVReader\")\n",
    "\n",
    "# loader = PagedCSVReader(encoding=\"utf-8\")\n",
    "# csv_docs = loader.load_data(file=Path('data/halal_non_halal_ingred.csv'))\n",
    "\n",
    "# # print([x.doc_id for x in docs])\n",
    "# print(f\"Loaded {len(csv_docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49695133-be41-40a2-a379-6d4a11bfb984",
   "metadata": {},
   "source": [
    "## Creating an Index\n",
    "\n",
    "Once all the data is loaded, we can proceed to build an index for the chatbot. There are four types of indexing methods available: Summary Index, VectorStore Index, Tree Index, and Keyword Table Index. In this context, we will be utilizing the `VectorStore Index`, which happens to be one of the most widely used indexing techniques.\n",
    "\n",
    "### Step 1: Set Up OpenAI Service Context for NLP\n",
    "\n",
    "Next, set up the ServiceContext with OpenAI GPT-3.5-turbo for processing and understanding user queries.\n",
    "-  for more info on service context, refer to https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context.html#setting-global-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085980c4-7fc7-4856-a25e-60116e9ea387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('secrets.toml', 'r') as f:\n",
    "    config = toml.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae8cdc4-9c46-41df-8510-06c31782417a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "dotenv_path = './.env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Fetch and set the OpenAI API key\n",
    "try:\n",
    "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "except KeyError:\n",
    "    print(\"OpenAI API key not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d358ce-19a9-4033-81a1-5358af4a24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
    "\n",
    "#configure service context\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1187c-fb2a-4136-b608-d4d2a4d8e4b7",
   "metadata": {},
   "source": [
    "### Step 2: Set Up Document Indexing - Storing your index\n",
    "\n",
    "First, create and store the document index if it doesn't already exist, or load it if it does. This index will be used to retrieve information about ingredients.\n",
    "- for more information on storing your index, refer to: https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac3be1c-bf8d-4515-af47-cbffbd1668f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c2ed935d5e4388bf1ac2d10ad74ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7c4b41e4484568aa3011c10adba3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and persisted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Directory to store the indexed data\n",
    "storage_dir = \"./storage\"\n",
    "\n",
    "# Ensure the storage directory exists\n",
    "if not os.path.exists(storage_dir):\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "    \n",
    "# Create a StorageContext\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "\n",
    "# Create the VectorStoreIndex with service_context\n",
    "index = GPTVectorStoreIndex.from_documents(pdfhtml_docs, service_context=gpt_context, show_progress=True)\n",
    "\n",
    "# Persist the index\n",
    "index.storage_context.persist()\n",
    "print(\"Index created and persisted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabf2ea-98c4-4f23-864a-ab81f6d30e3a",
   "metadata": {},
   "source": [
    "### Technically speaking, we are already able to proceed with a chat engine and end off this notebook. But, I am curious to know how to fine tune, so here it goes. \n",
    "\n",
    "The next step is generating a training and eval dataset.\n",
    "\n",
    "We will generate 40 questions on different sections of the docs we just ingested.\n",
    "\n",
    "Then, we will use GPT-3.5 on the eval questions to get our baseline performance, followed by using GPT-4 on the train questions to generate our training data. The training data will be collected with out `OpenAIFineTuningHandler`.\n",
    "\n",
    "---\n",
    "\n",
    "more info here: https://gpt-index.readthedocs.io/en/v0.8.49/examples/finetuning/openai_fine_tuning.html (this is where I got most of my code from too) \n",
    "\n",
    "Note: considerable amount of money and time is spent here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2c65a-8a10-4be9-b269-8a8623997531",
   "metadata": {},
   "source": [
    "## Train generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3ae894-2cf3-4fe8-814f-6e4a17467a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "random.seed(42)\n",
    "random.shuffle(pdfhtml_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefdf0c3-3428-4b2d-8909-d9c023fca682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:187: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    }
   ],
   "source": [
    "# Define the question generation query\n",
    "# This query will guide the generation of questions for each document\n",
    "# It's focused on evaluating the halal status of ingredients in food products\n",
    "\n",
    "question_gen_query = (\n",
    "    \"Given your expertise in Halal food certification, I need you to analyze the ingredients of a specific food product for Halal compliance. Please provide a clear and detailed assessment for each ingredient based on the information available in the provided documents. Consider factors such as the source and processing methods of each ingredient, as these can impact its Halal status. If any ingredient is non-Halal, doubtful, or lacks sufficient information for assessment, please categorize the entire food product accordingly. Your goal is to determine the overall Halal status of the food product. Please provide concise and factually accurate responses.\"\n",
    ")\n",
    "\n",
    "# find out more about question generation from \n",
    "# https://gpt-index.readthedocs.io/en/latest/examples/evaluation/QuestionGeneration.html\n",
    "\n",
    "# Create the dataset generator using the combined and shuffled documents\n",
    "# This will use the defined service context and question generation query\n",
    "# to create a dataset where each document is paired with a generated question\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    pdfhtml_docs[5:10],   # Limit documents to ratelimit\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_context,  # Use the previously defined service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca3e39a1-2084-4d0e-94f7-13f70b630380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To avoid RuntimeError: asyncio.run() cannot be called from a running event loop\n",
    "# The below code is to unblock: nest the event loops\n",
    "# Apply asyncio patch to enable asynchronous operations in a Jupyter environment\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8717c9e6-b52c-4e71-980f-2b05ec2440ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:282: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "# Set the timeout (in seconds)\n",
    "# openai.api_timeout = 80 \n",
    "\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9791d8-852f-46e4-8a68-137f4075614c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is Rennet Casein considered Halal?', 'Is Riboflavin considered Halal?', 'Is Rice considered Halal?', 'Is Rosemary Extract considered Halal?', 'Is Rum considered Halal?', 'Is Rye considered Halal?', 'Is Saccharine considered Halal?', 'Is Safflower considered Halal?', 'Is Saffron considered Halal?', 'Is Sage considered Halal?', 'Is Salt considered Halal?', 'Is Savory considered Halal?', 'Is Sea Salt considered Halal?', 'Is Semolina considered Halal?', 'Is Durum Wheat Flour considered Halal?', 'Is Sesame considered Halal?', 'Is Shallot considered Halal?', 'Is Shellac considered Halal?', 'Is Sherry Wine considered Halal?', 'Is Silicon Dioxide considered Halal?', 'Is Sodium Acetate considered Halal?', 'Is Sodium Acid Pyrophosphate considered Halal?', 'Is Sodium Aluminum Phosphate considered Halal?', 'Is Sodium Aluminum Sulfate considered Halal?', 'Is Sodium Benzoate considered Halal?', 'Is Sodium Bicarbonate considered Halal', 'What is the Halal status of Curcumin (C.I. 75300)?', 'Is Riboflavin/Lactofavin/Vitamin B2 Halal?', 'Can Tartrazine/FD&C Yellow 5 (C.I. 19140) be considered Halal?', 'Is Quinoline Yellow (C.I. 47005) Halal?', 'What is the Halal status of Yellow 2G (C.I. 18965)?', 'Can Sunset Yellow FCF/FD&C Yellow 6 (C.I. 15985) be considered Halal?', 'Is Cochineal/Carmines (C.I. 75470) Halal?', 'What is the Halal status of Carmoisine/Azorubine (C.I. 14720)?', 'Can Amaranth/FD&C Red 2 (C.I. 16185) be considered Halal?', 'Is Ponceau 4R/Cochineal Red A (C.I. 16255) Halal?', 'What is the Halal status of Erythrosine/FD&C Red 3 (C.I. 45430)?', 'Can', 'What is the Halal status of Colouring ingredient 40850?', 'Is Beet Red/Betanin/Betanidin (Colouring) considered Halal?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4003b78-8f54-497d-8003-9259439b832f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open a file named 'train_questions.txt' in write mode\n",
    "# 'with open' is used for safe handling of file operations\n",
    "with open(\"train_questions.txt\", \"w\") as f:\n",
    "    # Iterate over each question in the 'questions' list\n",
    "    for question in questions:\n",
    "        # Write each question to the file, followed by a newline character\n",
    "        # This ensures each question is on a new line in the file\n",
    "        f.write(question + \"\\n\")\n",
    "        # The newline character '\\n' is important for separating the questions\n",
    "\n",
    "# Note: The 'with open' statement automatically handles the closing of the file\n",
    "# once the block of code under it is executed. This is a good practice to prevent\n",
    "# file handling errors and ensure that data is properly written to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516d983-8837-44ce-9db1-f091c3f37193",
   "metadata": {},
   "source": [
    "### Generate Evaluation Dataset\n",
    "\n",
    "This dataset is for subsequent evaluation step to measure the performance of the models.\n",
    "<br> Questions are generated from a different set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867af7b6-24c3-4f72-86e0-b93322a97123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:187: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    }
   ],
   "source": [
    "# Create a DatasetGenerator from a subset of 'docs' starting from the 4th document (index 3)\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    pdfhtml_docs[\n",
    "        15:18],  # since we generated question for the first 5 documents, we can skip the documents we generated questions from \n",
    "    question_gen_query=question_gen_query,  # Specify the question generation query\n",
    "    service_context=gpt_context,  # Provide the GPT service context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0bc781e-f7f3-443f-a774-7c12afe3f7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  20  questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:282: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "# Generate questions from a dataset using the dataset generator\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=20)\n",
    "\n",
    "# Print the number of generated questions\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01203890-9e1b-46a1-8b7f-0a2ceda236b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write the generated questions to a file for evaluation purpose\n",
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e4ea062-4108-4e2b-9851-cf1808304532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 51\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents:\", len(pdfhtml_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580652af-7d67-42ed-bf10-eb808bd2a547",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo to Generate Training Data\n",
    "\n",
    "This code is used to set up a fine-tuning process for a language model, specifically GPT-3.5 Turbo. Here's a breakdown of what it does:\n",
    "\n",
    "- Create an instance of the `OpenAIFineTuningHandler`. This handler is used for fine-tuning the language model.\n",
    "\n",
    "- Create a `CallbackManager` to manage callbacks during model interactions. The fine-tuning handler is added to this manager. Callbacks are functions that can be executed at various points during model operations.\n",
    "\n",
    "- Configure the GPT-3.5 Turbo model with a specific context. This context includes the following settings:\n",
    "    - The language model used is GPT-3.5 Turbo.\n",
    "    - The temperature parameter is set to 0, which means deterministic output (no randomness).\n",
    "    - The context window is limited to 2048 tokens. This artificially limits the amount of context that the model can consider, possibly for testing or optimization purposes.\n",
    "    - The callback_manager is set to the previously created callback_manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326fb496-5c27-4501-a7e7-b1966972d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "# Create an instance of the OpenAIFineTuningHandler for fine-tuning\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "\n",
    "# Create a CallbackManager and add the fine-tuning handler to it\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "# Configure the GPT-3.5 Turbo model with a specific context\n",
    "gpt_3_5_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0e453b0-6ed7-43e8-9521-8a18fa0a8a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read questions from a file and store them in a list\n",
    "\n",
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b049a74b-40e4-4a1c-9247-aa3b82730cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a VectorStoreIndex from a list of documents using the gpt_4_context\n",
    "index = VectorStoreIndex.from_documents(pdfhtml_docs, service_context=gpt_3_5_context)\n",
    "\n",
    "# Create a query engine based on the index with a specified similarity threshold\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e73c1ef-9ef1-4c63-8c8d-8ffd27cfa52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through a list of questions and query the query engine for each question\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153199db-26d2-4465-a13a-25f3709efca3",
   "metadata": {},
   "source": [
    "### Create `OpenAIFinetuneEngine`\n",
    "\n",
    "`OpenAIFinetuneEngine` is a finetune engine that will take care of launching a finetuning job, and returning an LLM model that can be directly plugged in to the rest of LlamaIndex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2c16a52-69bd-4865-a423-208d34f19d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 73 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuning events to a JSONL file\n",
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1800c-5bc3-45c3-ac20-a9a4cd504dcc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To measure the performance of the pipeline, whether it is able to generate relevant and accurate responses given the external data source and a set of queries, we use 2 evaluation metrics from [`ragas` evaluation library](https://github.com/explodinggradients/ragas/tree/main/docs/concepts/metrics). Ragas uses LLMs under the hood to compute the evaluations.\n",
    "\n",
    "The performance of the base model, gpt-3.5-turbo, will be compared with the fine-tuned model.\n",
    "\n",
    "Computation of evaluation metrics require 3 components: \n",
    "1) `Question`: A list of questions that could be asked about my external data/documents, generated using .generate_questions_from_nodes in above fine-tuning step<br>\n",
    "2) `Context`: Retrieved contexts corresponding to each question. The context represents (chunks of) documents that are relevant to the question, i.e. the source from where the answer will be generated.<br>\n",
    "3) `Answer`: Answer generated corresponding to each question from baseline and fine-tuned model.\n",
    "\n",
    "The two metrics are as follow:\n",
    "\n",
    "- `answer_relevancy` - Measures how relevant the generated answer is to the question, where an answer is considered relevant when it <u>directly</u> and <u>appropriately</u> addresses the orginal question, i.e. answers that are complete and do not include unnecessary or duplicated information. The metric does not consider factuality. It is computed using `question` and `answer`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing relevant answers. To calculate this score, the LLM is prompted to generate an appropriate question for the generated answer multiple times, and the mean cosine similarity between these generated questions and the original question is measured. The underlying idea is that if the generated answer accurately addresses the initial question, the LLM should be able to generate questions from the answer that align with the original question, i.e. high mean cosine similarity, translating to high score.\n",
    "\n",
    "\n",
    "- `faithfulness` - Measures how factually accurate is the generated answer, i.e. if the response was hallucinated, or based on factuality (from the context). It is computed from `answer` and `context`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing contextually accurate information. To calculate this score, the LLM identifies statements within the generated answer and verifies if each statement is supported by the retrieved context. The process then counts the number of statements within the generated answer that can be logically inferred from the context, and dvide by the total number of statements in the answer. \n",
    "\n",
    "Additional note: Cosine similarity is a metric used to measure how similar two items are. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The output value ranges from 0–1 where 0 means no similarity, whereas 1 means that both the items are 100% similar.\n",
    "<br>Hallucinations refer to instances where the language model produces information or claims that are not accurate or supported by the input context.\n",
    "\n",
    "Resources:\n",
    "<br>https://cobusgreyling.medium.com/rag-evaluation-9813a931b3d4\n",
    "<br>https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\n",
    "<br>https://medium.aiplanet.com/evaluate-rag-pipeline-using-ragas-fbdd8dd466c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7e74ded-7b06-4efd-b849-82f3e691d325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48e82c88-46f4-4883-8a7a-4f04e573fc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    # If finetuning on openai website, replace the model name accordingly\n",
    "    llm=OpenAI(model=\"ft:gpt-3.5-turbo-1106:personal::8TJn8Zjj\", temperature=0), context_window=2048\n",
    "    \n",
    "    # If finetuning on localhost, uncomment this code\n",
    "    # llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(pdfhtml_docs, service_context=gpt_context)\n",
    "\n",
    "# as_query_engine builds a default retriever and query engine on top of the index\n",
    "# We configure the retriever to return the top 2 most similar documents, which is also the default setting\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e64d30a1-1ffb-40f1-81a7-48ad549bcb05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists to store contexts and answers\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "# Iterate through a list of questions\n",
    "for question in questions:\n",
    "    # Query the query_engine with the current question\n",
    "    response = query_engine.query(question)\n",
    "    \n",
    "    # Extract and store the content of source nodes as contexts\n",
    "    # This assumes that response.source_nodes is a list of nodes\n",
    "    # and each node has a get_content() method\n",
    "    context_content = [x.node.get_content() for x in response.source_nodes]\n",
    "    contexts.append(context_content)\n",
    "    \n",
    "    # Convert the response to a string and store it as an answer\n",
    "    answer_str = str(response)\n",
    "    answers.append(answer_str)\n",
    "\n",
    "# At the end of this loop, 'contexts' will contain lists of context content,\n",
    "# and 'answers' will contain the responses generated by the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6100781e-e747-4d9e-b340-bef279615dad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:27<00:00, 13.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 0.9692, 'faithfulness': 0.8500}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "# Create a Dataset from a dictionary containing questions, answers, and contexts\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,  # List of questions\n",
    "        \"answer\": answers,      # List of answers\n",
    "        \"contexts\": contexts,  # List of contexts\n",
    "    }\n",
    ")\n",
    "\n",
    "# Evaluate the dataset using specified metrics (faithfulness and answer_relevancy)\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "\n",
    "# Print the evaluation result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b92f5769-c6f7-44c7-9527-5ee4f17d87cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_gpt_35 = result.to_pandas()\n",
    "\n",
    "# Export cleaned dataframe as .csv\n",
    "df_gpt_35.to_csv(\"df_gpt_35.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fa7de-dcd4-4d32-80fe-8ade213b4735",
   "metadata": {},
   "source": [
    "Retrying with gpt-3.5-turbo-0613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cee607b-116a-4754-85d4-4c3b27d97229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32854034-6a3c-4fb8-a904-821c499d592c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    # If finetuning on openai website, replace the model name accordingly\n",
    "    llm=OpenAI(model=\"ft:gpt-3.5-turbo-0613:personal::8TKpiiVo\", temperature=0), context_window=2048\n",
    "    \n",
    "    # If finetuning on localhost, uncomment this code\n",
    "    # llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(pdfhtml_docs, service_context=gpt_context)\n",
    "\n",
    "# as_query_engine builds a default retriever and query engine on top of the index\n",
    "# We configure the retriever to return the top 2 most similar documents, which is also the default setting\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e6b9d17-e819-4411-b7ca-b7bf8a3f3b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists to store contexts and answers\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "# Iterate through a list of questions\n",
    "for question in questions:\n",
    "    # Query the query_engine with the current question\n",
    "    response = query_engine.query(question)\n",
    "    \n",
    "    # Extract and store the content of source nodes as contexts\n",
    "    # This assumes that response.source_nodes is a list of nodes\n",
    "    # and each node has a get_content() method\n",
    "    context_content = [x.node.get_content() for x in response.source_nodes]\n",
    "    contexts.append(context_content)\n",
    "    \n",
    "    # Convert the response to a string and store it as an answer\n",
    "    answer_str = str(response)\n",
    "    answers.append(answer_str)\n",
    "\n",
    "# At the end of this loop, 'contexts' will contain lists of context content,\n",
    "# and 'answers' will contain the responses generated by the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19d81b89-5889-4c14-a8c5-b79d7b69fbed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:21<00:00, 10.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:29<00:00, 14.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 0.9590, 'faithfulness': 0.7500}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "# Create a Dataset from a dictionary containing questions, answers, and contexts\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,  # List of questions\n",
    "        \"answer\": answers,      # List of answers\n",
    "        \"contexts\": contexts,  # List of contexts\n",
    "    }\n",
    ")\n",
    "\n",
    "# Evaluate the dataset using specified metrics (faithfulness and answer_relevancy)\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "\n",
    "# Print the evaluation result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516b5a1-8d5a-4dbb-87f2-0378ef07d51b",
   "metadata": {
    "tags": []
   },
   "source": [
    "| Prompts                          | Models              | Answer Relevancy | Faithfulness |\n",
    "|----------------------------------|---------------------|------------------|--------------|\n",
    "| General                          | gpt-3.5-turbo-1106 | 0.969            | 0.8          |\n",
    "| General                          | gpt-3.5-turbo-0613  | 0.9591           | 0.75         |\n",
    "| Task Oriented                    | gpt-3.5-turbo-0613  | 0.944            | 0.6          |\n",
    "| Task (CSV, pdf, htm)             | gpt-3.5-turbo-0613  | 0.961            | 0.55         |\n",
    "\n",
    "\n",
    "The `gpt-3.5-turbo-1106 model` consistently outperforms `gpt-3.5-turbo-0613` across various prompt types, delivering higher answer relevancy and faithfulness. **General** prompts yield better results than task-oriented prompts. Careful selection of both model and prompt type is crucial for obtaining desired responses in specific applications.\n",
    "\n",
    "In addition to the previous conclusion, it's worth noting that the performance of the model is influenced by document formats. When all documents are in the PDF/HTML format, the model tends to perform better compared to scenarios involving a mix of PDF/HTML and CSV files. The variation in file formats significantly impacts model performance, highlighting the importance of consistent document types for optimal results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691eb1b-0330-4f36-869a-80730879a860",
   "metadata": {},
   "source": [
    "Backend- processing documents for reference\n",
    "\n",
    "- [Task(CSV,pdf,htm)](https://github.com/rasyidahbr/halal_non_halal_analysis/blob/main/cd/backend_processing.ipynb)\n",
    "- [Task Oriented](https://github.com/rasyidahbr/halal_non_halal_analysis/blob/main/task/backend_processing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
