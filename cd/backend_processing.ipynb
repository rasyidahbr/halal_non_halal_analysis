{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6f6064-b370-4154-8fc9-f5aad10cdacb",
   "metadata": {},
   "source": [
    "# Building a simple chatbot with Retrieval Augmented Generation (RAG) using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55df4a-44bb-4a8a-9fb6-92501c7bbdd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installing the required packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a039957-b567-4961-9d4f-a64e42418921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install openai\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install pypdf\n",
    "# !pip install optimum[exporters]\n",
    "# !pip install InstructorEmbedding\n",
    "# !pip install sentence_transformers\n",
    "# !pip install python-dotenv\n",
    "# !pip install ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f763f9-06f9-4f85-ab15-4926e34b88ef",
   "metadata": {},
   "source": [
    "## Import libraries, API and set filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cbe370-a693-493c-bd9d-9ad80a257a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document, GPTVectorStoreIndex, ServiceContext\n",
    "from llama_index.readers import BeautifulSoupWebReader, SimpleDirectoryReader\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index import download_loader # For CSV\n",
    "from llama_index import (VectorStoreIndex,\n",
    "                         SimpleDirectoryReader,\n",
    "                         StorageContext,\n",
    "                         load_index_from_storage,)\n",
    "from llama_index import (ServiceContext, \n",
    "                         LLMPredictor, \n",
    "                         OpenAIEmbedding, \n",
    "                         PromptHelper)\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index import set_global_service_context\n",
    "# from llama_index.llama_dataset.generator import RagDatasetGenerator\n",
    "# from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import nest_asyncio\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687537b5-d096-4c9b-ad9c-e013fdbf1386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set filepath to my data directory \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, \"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2fe4b-d17e-46ba-93ec-d4122dc655bc",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "According to [LlamaIndex's documentation](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/simple_directory_reader.html), the `SimpleDirectoryReader` is the most commonly used data connector that just works. Simply pass in a input directory or a list of files. It will select the best file reader based on the file extensions. \n",
    "\n",
    "In this use case here, there are PDFs and html pages from the latest release from MUIS and other islamic wesbites, which are not included in gpt-3.5-turbo's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9282ea0-148b-40db-ae7f-542028a252c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18373dff-cffe-4659-8f36-a6fba23aa3fa', 'dda997ab-d7e2-4572-9988-b394dab7b6dd', '817622b8-856a-4be8-8dd6-32c17235505d', '9f22bed0-4c0b-43fa-9e07-36062aff9985', 'b500c5a1-1e6d-4f94-91ce-43ffd5578955', '468a716d-85c5-4d16-9e41-c1f28546fa50', 'c4873d43-f029-472b-a841-f0ba0def4f76', 'eb3456c1-419e-44e1-89cb-6e2cd9775705', '178518d2-8bd3-4fef-8e76-4fb5be75c3ed', '83a4fe0f-b419-46c6-bd2c-0d8226647f18', 'ea44417e-d3e8-437e-8a75-06c73b8d8e2a', 'eeda0439-d17e-4b93-897e-af73b2b23753', '93dc566e-4473-46c1-9f3f-a36444edbc1d', '33f224fa-b2b7-4b3b-a0bb-d322fdffb43c', 'a9d7051b-8fb3-43e4-bf49-d1591decd4f1', 'c2ebc973-9d75-4722-a7b8-6d3724aa17ab', 'b9640ce5-4825-4ecf-8585-710eb4bdca01', 'e18a0d4a-3b73-48a1-b99f-668208c1664b', 'c8bab785-e06e-47eb-abe4-61777972b52b', 'c9d119e7-5c0f-455c-a9b5-2daeb9099d0f', 'a1be2e71-a9a8-43d0-88da-166f99aa10f0', '02ff685d-94df-42a3-9e82-d1b71de02234', 'd4362531-6979-49c7-8037-ed0c573f28d6', 'ca066b14-bff9-44ed-ac09-3610ad9f6742', '25d61269-18c5-4f0d-9f2c-4bc26b3f6c39', '9cb787eb-4365-47a2-bd5e-a7328aa70a79', '1128e069-e5ce-4dd5-92ce-fc16e4b1fd44']\n"
     ]
    }
   ],
   "source": [
    "filename_fn = lambda filename: {'file_name': filename}\n",
    "pdfhtml_docs = SimpleDirectoryReader(input_dir=data_dir, exclude_hidden=True, file_metadata=filename_fn).load_data()\n",
    "print([x.doc_id for x in pdfhtml_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12628f87-30ba-48f4-bcf2-6cba4719fdc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27 documents.\n"
     ]
    }
   ],
   "source": [
    "# Check if documents are loaded\n",
    "if not pdfhtml_docs:\n",
    "    print(\"No documents loaded. Check your data directory path.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(pdfhtml_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e9021-78e5-45d1-b885-49b8eb2d8540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 886 docs\n"
     ]
    }
   ],
   "source": [
    "#Use this code if your data set is in CSV format\n",
    "PagedCSVReader = download_loader(\"PagedCSVReader\")\n",
    "\n",
    "loader = PagedCSVReader(encoding=\"utf-8\")\n",
    "csv_docs = loader.load_data(file=Path('data/halal_non_halal_ingred.csv'))\n",
    "\n",
    "# print([x.doc_id for x in docs])\n",
    "print(f\"Loaded {len(csv_docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49695133-be41-40a2-a379-6d4a11bfb984",
   "metadata": {},
   "source": [
    "## Creating an Index\n",
    "\n",
    "Once all the data is loaded, we can proceed to build an index for the chatbot. There are four types of indexing methods available: Summary Index, VectorStore Index, Tree Index, and Keyword Table Index. In this context, we will be utilizing the `VectorStore Index`, which happens to be one of the most widely used indexing techniques.\n",
    "\n",
    "### Step 1: Set Up OpenAI Service Context for NLP\n",
    "\n",
    "Next, set up the ServiceContext with OpenAI GPT-3.5-turbo for processing and understanding user queries.\n",
    "-  for more info on service context, refer to https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context.html#setting-global-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085980c4-7fc7-4856-a25e-60116e9ea387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('secrets.toml', 'r') as f:\n",
    "    config = toml.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae8cdc4-9c46-41df-8510-06c31782417a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "dotenv_path = './.env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Fetch and set the OpenAI API key\n",
    "try:\n",
    "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "except KeyError:\n",
    "    print(\"OpenAI API key not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d358ce-19a9-4033-81a1-5358af4a24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
    "\n",
    "#configure service context\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1187c-fb2a-4136-b608-d4d2a4d8e4b7",
   "metadata": {},
   "source": [
    "### Step 2: Set Up Document Indexing - Storing your index\n",
    "\n",
    "First, create and store the document index if it doesn't already exist, or load it if it does. This index will be used to retrieve information about ingredients.\n",
    "- for more information on storing your index, refer to: https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac3be1c-bf8d-4515-af47-cbffbd1668f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec9d06fb76c434c9fc6ec7741827c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/913 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f0dade75734e6a9925e74b2c10e3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and persisted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Directory to store the indexed data\n",
    "storage_dir = \"./storage\"\n",
    "\n",
    "# Ensure the storage directory exists\n",
    "if not os.path.exists(storage_dir):\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "    \n",
    "# Create a StorageContext\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "\n",
    "# Create the VectorStoreIndex with service_context\n",
    "comb_docs = pdfhtml_docs + csv_docs\n",
    "index = GPTVectorStoreIndex.from_documents(comb_docs, service_context=gpt_context, show_progress=True)\n",
    "\n",
    "# Persist the index\n",
    "index.storage_context.persist()\n",
    "print(\"Index created and persisted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabf2ea-98c4-4f23-864a-ab81f6d30e3a",
   "metadata": {},
   "source": [
    "### Technically speaking, we are already able to proceed with a chat engine and end off this notebook. But, I am curious to know how to fine tune, so here it goes. \n",
    "\n",
    "The next step is generating a training and eval dataset.\n",
    "\n",
    "We will generate 40 questions on different sections of the docs we just ingested.\n",
    "\n",
    "Then, we will use GPT-3.5 on the eval questions to get our baseline performance, followed by using GPT-4 on the train questions to generate our training data. The training data will be collected with out `OpenAIFineTuningHandler`.\n",
    "\n",
    "---\n",
    "\n",
    "more info here: https://gpt-index.readthedocs.io/en/v0.8.49/examples/finetuning/openai_fine_tuning.html (this is where I got most of my code from too) \n",
    "\n",
    "Note: considerable amount of money and time is spent here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2c65a-8a10-4be9-b269-8a8623997531",
   "metadata": {},
   "source": [
    "## Train generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3ae894-2cf3-4fe8-814f-6e4a17467a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "random.seed(42)\n",
    "random.shuffle(comb_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefdf0c3-3428-4b2d-8909-d9c023fca682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:187: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    }
   ],
   "source": [
    "# Define the question generation query\n",
    "# This query will guide the generation of questions for each document\n",
    "# It's focused on evaluating the halal status of ingredients in food products\n",
    "\n",
    "question_gen_query = (\n",
    "    \"As an expert in halal food certification, your task is to meticulously analyze the ingredients of food products. \"\n",
    "    \"A user is looking to determine if a specific food product is halal. \"\n",
    "    \"For each ingredient listed in a product, provide a detailed assessment based on the information available in the uploaded documents. \"\n",
    "    \"Some ingredients may have different halal statuses depending on their sources or processing methods. \"\n",
    "    \"Evaluate each ingredient to determine if it is halal, non-halal, or doubtful. \"\n",
    "    \"Consider the source and processing of each ingredient, as this can influence its halal status. \"\n",
    "    \"Your goal is to ensure that each ingredient in the product is halal. If any ingredient is non-halal, doubtful, not identifiable, or lacks sufficient information to ascertain its halal status, then the entire food product should be considered non-halal. \"\n",
    "    \"Provide a conclusive assessment for each ingredient, thereby determining the overall halal status of the food product.\"\n",
    ")\n",
    "\n",
    "# find out more about question generation from \n",
    "# https://gpt-index.readthedocs.io/en/latest/examples/evaluation/QuestionGeneration.html\n",
    "\n",
    "# Create the dataset generator using the combined and shuffled documents\n",
    "# This will use the defined service context and question generation query\n",
    "# to create a dataset where each document is paired with a generated question\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    comb_docs[:10],   # Limit to the first 40 documents\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_context,  # Use the previously defined service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca3e39a1-2084-4d0e-94f7-13f70b630380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To avoid RuntimeError: asyncio.run() cannot be called from a running event loop\n",
    "# The below code is to unblock: nest the event loops\n",
    "# Apply asyncio patch to enable asynchronous operations in a Jupyter environment\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8717c9e6-b52c-4e71-980f-2b05ec2440ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:282: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "# Set the timeout (in seconds)\n",
    "# openai.api_timeout = 80 \n",
    "\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9791d8-852f-46e4-8a68-137f4075614c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is acetone considered a halal ingredient?', 'What is the source of acetone?', 'How is acetone processed?', 'Does the source or processing method of acetone affect its halal status?', 'Is there any information available to determine the halal status of acetone?', 'Can acetone be considered non-halal or doubtful based on the available information?', 'Is there any reason to believe that acetone is not halal?', 'Is acetone identifiable as halal or non-halal based on the provided context information?', 'Does the halal status of acetone impact the overall halal status of the food product?', 'Can the food product be considered halal if acetone is determined to be non-halal or doubtful?', 'What is the halal status of the ingredient \"Rennet\"?', 'Is \"Rennet\" considered halal, non-halal, or doubtful?', 'Can the halal status of \"Rennet\" be determined based on the available information?', 'Does the source or processing method of \"Rennet\" affect its halal status?', 'Is there enough information to determine if \"Rennet\" is halal or non-halal?', 'What is the significance of the halal_non_halal_doubtful value of 2 for \"Rennet\"?', 'Is \"Rennet\" considered halal in all cases, or are there exceptions?', 'Can the overall halal status of the food product be determined if \"Rennet\" is doubtful?', 'If \"Rennet\" is non-halal, does it automatically make the entire food product non-halal?', 'What additional information is needed to determine the halal status of \"Rennet\"?', 'Is yeast extract from brewer yeast considered halal?', 'What is the source of the yeast extract from brewer yeast?', 'How is the yeast extract from brewer yeast processed?', 'Is there any information available about the halal status of yeast extract from brewer yeast?', 'Can the halal status of yeast extract from brewer yeast be determined based on the available information?', 'Is there any doubt regarding the halal status of yeast extract from brewer yeast?', 'Is yeast extract from brewer yeast considered non-halal?', 'Is the halal status of yeast extract from brewer yeast identifiable?', 'Does the lack of information about yeast extract from brewer yeast affect its halal status?', 'Does the halal status of yeast extract from brewer yeast impact the overall halal status of the food product?', 'Is eggplant considered halal?', 'What is the halal status of eggplant?', 'Can eggplant be classified as non-halal?', 'Is there any doubt regarding the halal status of eggplant?', 'Based on the available information, is eggplant halal, non-halal, or doubtful?', 'Does the source or processing method of eggplant affect its halal status?', 'Is there any specific information about eggplant that would determine its halal status?', 'Can the halal status of eggplant be determined based on the provided context information?', 'Is there any reason to believe that eggplant is not halal?', 'Is there any information missing that would help determine the halal status of eggplant?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4003b78-8f54-497d-8003-9259439b832f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open a file named 'train_questions.txt' in write mode\n",
    "# 'with open' is used for safe handling of file operations\n",
    "with open(\"train_questions.txt\", \"w\") as f:\n",
    "    # Iterate over each question in the 'questions' list\n",
    "    for question in questions:\n",
    "        # Write each question to the file, followed by a newline character\n",
    "        # This ensures each question is on a new line in the file\n",
    "        f.write(question + \"\\n\")\n",
    "        # The newline character '\\n' is important for separating the questions\n",
    "\n",
    "# Note: The 'with open' statement automatically handles the closing of the file\n",
    "# once the block of code under it is executed. This is a good practice to prevent\n",
    "# file handling errors and ensure that data is properly written to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516d983-8837-44ce-9db1-f091c3f37193",
   "metadata": {},
   "source": [
    "### Generate Evaluation Dataset\n",
    "\n",
    "This dataset is for subsequent evaluation step to measure the performance of the models.\n",
    "<br> Questions are generated from a different set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867af7b6-24c3-4f72-86e0-b93322a97123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:187: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    }
   ],
   "source": [
    "# Create a DatasetGenerator from a subset of 'docs' starting from the 4th document (index 3)\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    comb_docs[\n",
    "        10:20],  # since we generated question for the first 40 documents, we can skip the first 40 \n",
    "    question_gen_query=question_gen_query,  # Specify the question generation query\n",
    "    service_context=gpt_context,  # Provide the GPT service context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0bc781e-f7f3-443f-a774-7c12afe3f7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  20  questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myste\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\dataset_generation.py:282: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "# Generate questions from a dataset using the dataset generator\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=20)\n",
    "\n",
    "# Print the number of generated questions\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01203890-9e1b-46a1-8b7f-0a2ceda236b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write the generated questions to a file for evaluation purpose\n",
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e4ea062-4108-4e2b-9851-cf1808304532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 913\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents:\", len(comb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580652af-7d67-42ed-bf10-eb808bd2a547",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo to Generate Training Data\n",
    "\n",
    "This code is used to set up a fine-tuning process for a language model, specifically GPT-3.5 Turbo. Here's a breakdown of what it does:\n",
    "\n",
    "- Create an instance of the `OpenAIFineTuningHandler`. This handler is used for fine-tuning the language model.\n",
    "\n",
    "- Create a `CallbackManager` to manage callbacks during model interactions. The fine-tuning handler is added to this manager. Callbacks are functions that can be executed at various points during model operations.\n",
    "\n",
    "- Configure the GPT-3.5 Turbo model with a specific context. This context includes the following settings:\n",
    "    - The language model used is GPT-3.5 Turbo.\n",
    "    - The temperature parameter is set to 0, which means deterministic output (no randomness).\n",
    "    - The context window is limited to 2048 tokens. This artificially limits the amount of context that the model can consider, possibly for testing or optimization purposes.\n",
    "    - The callback_manager is set to the previously created callback_manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326fb496-5c27-4501-a7e7-b1966972d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "# Create an instance of the OpenAIFineTuningHandler for fine-tuning\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "\n",
    "# Create a CallbackManager and add the fine-tuning handler to it\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "# Configure the GPT-3.5 Turbo model with a specific context\n",
    "gpt_3_5_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0e453b0-6ed7-43e8-9521-8a18fa0a8a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read questions from a file and store them in a list\n",
    "\n",
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad476d2d-5571-45ec-976a-d508f4e48126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is acetone considered a halal ingredient?',\n",
       " 'What is the source of acetone?',\n",
       " 'How is acetone processed?',\n",
       " 'Does the source or processing method of acetone affect its halal status?',\n",
       " 'Is there any information available to determine the halal status of acetone?',\n",
       " 'Can acetone be considered non-halal or doubtful based on the available information?',\n",
       " 'Is there any reason to believe that acetone is not halal?',\n",
       " 'Is acetone identifiable as halal or non-halal based on the provided context information?',\n",
       " 'Does the halal status of acetone impact the overall halal status of the food product?',\n",
       " 'Can the food product be considered halal if acetone is determined to be non-halal or doubtful?',\n",
       " 'What is the halal status of the ingredient \"Rennet\"?',\n",
       " 'Is \"Rennet\" considered halal, non-halal, or doubtful?',\n",
       " 'Can the halal status of \"Rennet\" be determined based on the available information?',\n",
       " 'Does the source or processing method of \"Rennet\" affect its halal status?',\n",
       " 'Is there enough information to determine if \"Rennet\" is halal or non-halal?',\n",
       " 'What is the significance of the halal_non_halal_doubtful value of 2 for \"Rennet\"?',\n",
       " 'Is \"Rennet\" considered halal in all cases, or are there exceptions?',\n",
       " 'Can the overall halal status of the food product be determined if \"Rennet\" is doubtful?',\n",
       " 'If \"Rennet\" is non-halal, does it automatically make the entire food product non-halal?',\n",
       " 'What additional information is needed to determine the halal status of \"Rennet\"?',\n",
       " 'Is yeast extract from brewer yeast considered halal?',\n",
       " 'What is the source of the yeast extract from brewer yeast?',\n",
       " 'How is the yeast extract from brewer yeast processed?',\n",
       " 'Is there any information available about the halal status of yeast extract from brewer yeast?',\n",
       " 'Can the halal status of yeast extract from brewer yeast be determined based on the available information?',\n",
       " 'Is there any doubt regarding the halal status of yeast extract from brewer yeast?',\n",
       " 'Is yeast extract from brewer yeast considered non-halal?',\n",
       " 'Is the halal status of yeast extract from brewer yeast identifiable?',\n",
       " 'Does the lack of information about yeast extract from brewer yeast affect its halal status?',\n",
       " 'Does the halal status of yeast extract from brewer yeast impact the overall halal status of the food product?',\n",
       " 'Is eggplant considered halal?',\n",
       " 'What is the halal status of eggplant?',\n",
       " 'Can eggplant be classified as non-halal?',\n",
       " 'Is there any doubt regarding the halal status of eggplant?',\n",
       " 'Based on the available information, is eggplant halal, non-halal, or doubtful?',\n",
       " 'Does the source or processing method of eggplant affect its halal status?',\n",
       " 'Is there any specific information about eggplant that would determine its halal status?',\n",
       " 'Can the halal status of eggplant be determined based on the provided context information?',\n",
       " 'Is there any reason to believe that eggplant is not halal?',\n",
       " 'Is there any information missing that would help determine the halal status of eggplant?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b049a74b-40e4-4a1c-9247-aa3b82730cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a VectorStoreIndex from a list of documents using the gpt_4_context\n",
    "index = VectorStoreIndex.from_documents(comb_docs, service_context=gpt_3_5_context)\n",
    "\n",
    "# Create a query engine based on the index with a specified similarity threshold\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e73c1ef-9ef1-4c63-8c8d-8ffd27cfa52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through a list of questions and query the query engine for each question\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153199db-26d2-4465-a13a-25f3709efca3",
   "metadata": {},
   "source": [
    "### Create `OpenAIFinetuneEngine`\n",
    "\n",
    "`OpenAIFinetuneEngine` is a finetune engine that will take care of launching a finetuning job, and returning an LLM model that can be directly plugged in to the rest of LlamaIndex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2c16a52-69bd-4865-a423-208d34f19d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 40 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuning events to a JSONL file\n",
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1800c-5bc3-45c3-ac20-a9a4cd504dcc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To measure the performance of the pipeline, whether it is able to generate relevant and accurate responses given the external data source and a set of queries, we use 2 evaluation metrics from [`ragas` evaluation library](https://github.com/explodinggradients/ragas/tree/main/docs/concepts/metrics). Ragas uses LLMs under the hood to compute the evaluations.\n",
    "\n",
    "The performance of the base model, gpt-3.5-turbo, will be compared with the fine-tuned model.\n",
    "\n",
    "Computation of evaluation metrics require 3 components: \n",
    "1) `Question`: A list of questions that could be asked about my external data/documents, generated using .generate_questions_from_nodes in above fine-tuning step<br>\n",
    "2) `Context`: Retrieved contexts corresponding to each question. The context represents (chunks of) documents that are relevant to the question, i.e. the source from where the answer will be generated.<br>\n",
    "3) `Answer`: Answer generated corresponding to each question from baseline and fine-tuned model.\n",
    "\n",
    "The two metrics are as follow:\n",
    "\n",
    "- `answer_relevancy` - Measures how relevant the generated answer is to the question, where an answer is considered relevant when it <u>directly</u> and <u>appropriately</u> addresses the orginal question, i.e. answers that are complete and do not include unnecessary or duplicated information. The metric does not consider factuality. It is computed using `question` and `answer`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing relevant answers. To calculate this score, the LLM is prompted to generate an appropriate question for the generated answer multiple times, and the mean cosine similarity between these generated questions and the original question is measured. The underlying idea is that if the generated answer accurately addresses the initial question, the LLM should be able to generate questions from the answer that align with the original question, i.e. high mean cosine similarity, translating to high score.\n",
    "\n",
    "\n",
    "- `faithfulness` - Measures how factually accurate is the generated answer, i.e. if the response was hallucinated, or based on factuality (from the context). It is computed from `answer` and `context`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing contextually accurate information. To calculate this score, the LLM identifies statements within the generated answer and verifies if each statement is supported by the retrieved context. The process then counts the number of statements within the generated answer that can be logically inferred from the context, and dvide by the total number of statements in the answer. \n",
    "\n",
    "Additional note: Cosine similarity is a metric used to measure how similar two items are. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The output value ranges from 0–1 where 0 means no similarity, whereas 1 means that both the items are 100% similar.\n",
    "<br>Hallucinations refer to instances where the language model produces information or claims that are not accurate or supported by the input context.\n",
    "\n",
    "Resources:\n",
    "<br>https://cobusgreyling.medium.com/rag-evaluation-9813a931b3d4\n",
    "<br>https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\n",
    "<br>https://medium.aiplanet.com/evaluate-rag-pipeline-using-ragas-fbdd8dd466c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7e74ded-7b06-4efd-b849-82f3e691d325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48e82c88-46f4-4883-8a7a-4f04e573fc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    # If finetuning on openai website, replace the model name accordingly\n",
    "    llm=OpenAI(model=\"ft:gpt-3.5-turbo-0613:personal::8SPZ7tgW\", temperature=0), context_window=2048\n",
    "    \n",
    "    # If finetuning on localhost, uncomment this code\n",
    "    # llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(comb_docs, service_context=gpt_context)\n",
    "\n",
    "# as_query_engine builds a default retriever and query engine on top of the index\n",
    "# We configure the retriever to return the top 2 most similar documents, which is also the default setting\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e64d30a1-1ffb-40f1-81a7-48ad549bcb05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists to store contexts and answers\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "# Iterate through a list of questions\n",
    "for question in questions:\n",
    "    # Query the query_engine with the current question\n",
    "    response = query_engine.query(question)\n",
    "    \n",
    "    # Extract and store the content of source nodes as contexts\n",
    "    # This assumes that response.source_nodes is a list of nodes\n",
    "    # and each node has a get_content() method\n",
    "    context_content = [x.node.get_content() for x in response.source_nodes]\n",
    "    contexts.append(context_content)\n",
    "    \n",
    "    # Convert the response to a string and store it as an answer\n",
    "    answer_str = str(response)\n",
    "    answers.append(answer_str)\n",
    "\n",
    "# At the end of this loop, 'contexts' will contain lists of context content,\n",
    "# and 'answers' will contain the responses generated by the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6100781e-e747-4d9e-b340-bef279615dad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:45<00:00, 22.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 0.9610, 'faithfulness': 0.5500}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "# Create a Dataset from a dictionary containing questions, answers, and contexts\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,  # List of questions\n",
    "        \"answer\": answers,      # List of answers\n",
    "        \"contexts\": contexts,  # List of contexts\n",
    "    }\n",
    ")\n",
    "\n",
    "# Evaluate the dataset using specified metrics (faithfulness and answer_relevancy)\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "\n",
    "# Print the evaluation result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b92f5769-c6f7-44c7-9527-5ee4f17d87cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_gpt_35 = result.to_pandas()\n",
    "\n",
    "# Export cleaned dataframe as .csv\n",
    "df_gpt_35.to_csv(\"df_gpt_35.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96575f5-35d8-4603-ae86-ad1b65715a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
